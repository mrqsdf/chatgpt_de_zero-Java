# PXL-GPT: Build ChatGPT From Scratch

![Python](https://img.shields.io/badge/python-3.12-blue)
![License](https://img.shields.io/badge/license-MIT-green)

A custom implementation of a GPT model built from scratch. This project demonstrates the fundamental concepts behind large language models like ChatGPT by implementing each component step by step.

## ğŸŒŸ Features

- **BPE Tokenizer**: Custom implementation of Byte Pair Encoding for text tokenization
- **Dataset Handling**: Preprocessing and management of text datasets
- **Model Training**: Train your own GPT model on custom data
- **Clean Architecture**: Modular design for easy understanding and extension

## ğŸ› ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/chatgpt_from_scratch.git
cd chatgpt_from_scratch

# Install the package in development mode
pip install -e .

# Install pre-commit hooks
pre-commit install --install-hooks
```

## ğŸš€ Quick Start

### Training a Tokenizer

```python
from pxl_gpt.dataset import Dataset
from pxl_gpt.tokenizer import BPETokenizer

# Load and preprocess dataset
dataset = Dataset()
dataset.preprocess()

# Train tokenizer
tokenizer = BPETokenizer()
tokenizer.train(dataset)

# Save tokenizer
tokenizer.save("data/tokenizer.json")
```

### Testing the Tokenizer

```python
# Load a saved tokenizer
tokenizer = BPETokenizer()
tokenizer.load("data/tokenizer.json")

# Test encoding and decoding
test_text = "Bonjour, comment Ã§a va?"
encoded = tokenizer.encode(test_text)
decoded = tokenizer.decode(encoded)
print(f"Original: {test_text}")
print(f"Encoded: {encoded}")
print(f"Decoded: {decoded}")
```

## ğŸ“Š Project Structure

```
chatgpt_from_scratch/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cleaned_data.csv  # Preprocessed dataset
â”‚   â””â”€â”€ tokenizer.json    # Trained tokenizer
â”œâ”€â”€ pxl_gpt/
â”‚   â”œâ”€â”€ dataset.py        # Dataset handling
â”‚   â”œâ”€â”€ dataset_loader.py # Dataset loading utilities
â”‚   â”œâ”€â”€ model.py          # GPT model implementation
â”‚   â””â”€â”€ tokenizer.py      # BPE tokenizer implementation
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train_tokenizer.py # Script to train the tokenizer
â””â”€â”€ pyproject.toml        # Project configuration
```

## ğŸ” Components

### Dataset

The dataset module handles loading and preprocessing text data. By default, it uses a French Reddit discussion dataset.

### Tokenizer

A custom implementation of Byte Pair Encoding (BPE) tokenization, similar to what's used in models like GPT. The tokenizer:

- Splits text into initial tokens
- Iteratively merges the most frequent adjacent token pairs
- Builds a vocabulary of subword units
- Provides encoding and decoding functionality

### Model (Coming Soon)

The GPT model architecture implementation.

## Translated Code 

- [Code en Java by MrQsdf](https://github.com/mrqsdf/chatgpt_de_zero-Java/tree/main) - Code translated to java

## ğŸ“š References

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - The original Transformer paper
- [Improving Language Understanding with Unsupervised Learning](https://openai.com/research/language-unsupervised) - OpenAI's GPT approach

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ‘¨â€ğŸ’» Author

Created by PixelCrafted
